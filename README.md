# LLM Alignment via Reinforcement Learning from Human Feedback (RLHF)

![Screenshot_2025-01-06_at_7 53 14_AM-removebg-preview](https://github.com/user-attachments/assets/d9e43213-3100-40c7-b2e7-f5b9ea0f9be6)

## Overview
This project demonstrates how to align Large Language Models (LLMs) using Reinforcement Learning from Human Feedback (RLHF). It includes training scripts for fine-tuning GPT-2 on the IMDB dataset to generate more positive movie reviews.

## Installation
To set up the project environment, follow these steps:
1. Install Python dependencies:
pip install -r requirements.txt

2. Run the main script:
python main.py


## Structure
- `data/`: Contains the dataset loading and preprocessing logic.
- `models/`: Contains the model definitions and training logic.
- `utils/`: Contains configuration settings and other utilities.
- `main.py`: The entry point of the project.

